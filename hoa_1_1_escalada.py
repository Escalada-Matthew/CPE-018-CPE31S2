# -*- coding: utf-8 -*-
"""HOA 1.1_Escalada

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kUrhFsGmkYyQ06eLyJWaAvKzjCzMPwrY

Technological Institute of the Philippines | Quezon City - Computer Engineering
--- | ---
Course Code: | CPE 018
Code Title: | Emerging Technologies in CpE 1 - Fundamentals of Computer Vision
1st Semester | AY 2025-2026
<hr> | <hr>
<u>**ACTIVITY NO. 1**</u> | <u>**NumPy and Pandas (Tutorial)**</u>
**Name** | Escalada, Lhei Matthew
**Section** | CPE31S2
**Date Performed**: 12/16/2025
**Date Submitted**: 12/16/2025
**Instructor**: | Engr. Maria Rizette H. Sayo

<hr>

## 1. Objectives

This activity aims to familiarize students with popular data processing tools, such as NumPy and Pandas, and apply them on sample data.

## 2. Intended Learning Outcomes

After this activity, the student should be able to:

* Utilize pandas and numpy in fundamental data analysis.
* Demonstrate data analysis operations using numpy and pandas.

## 3. Procedures and Outputs

### 3.1 Introduction
In this activity, a look at two importal tools will be done before jumping into using opencv and more advanced applications. We are looking at NumPy and Pandas!

From Pandas' website:

> *Pandas is a very popular library for working with data (its goal is to be the most powerful and flexible open-source tool, and in our opinion, it has reached that goal). DataFrames are at the center of pandas. A DataFrame is structured like a table or spreadsheet. The rows and the columns both have indexes, and you can perform operations on rows or columns separately.*

> *A pandas DataFrame can be easily changed and manipulated. Pandas has helpful functions for handling missing data, performing operations on columns and rows, and transforming data. If that wasn’t enough, a lot of SQL functions have counterparts in pandas, such as join, merge, filter by, and group by. With all of these powerful tools, it should come as no surprise that pandas is very popular among data scientists.*

<hr/>

From NumPy's website:
> *NumPy is an open-source Python library that facilitates efficient numerical operations on large quantities of data. There are a few functions that exist in NumPy that we use on pandas DataFrames. For us, the most important part about NumPy is that pandas is built on top of it. So, NumPy is a dependency of Pandas.*

<hr/>

*Disclaimer: Parts of this activity are sourced from https://endaq.com/ and other open source tutorials for data processing tools.*

#### 3.1.1 Recap of Python Introduction

1. Python is popular for good reason
2. There are many ways to interact with Python
  * Here we are in Google Colab based on Jupyter Notebooks
3. There are many open source libraries to use
  * Today we are covering two of the most popular:
    * Numpy
    * Pandas

<img src="https://info.endaq.com/hubfs/20210928-python-ide-libraries-inforgraphic-FINAL-2.png" width="700">

#### 3.1.2 SO MANY Other Resources
Python is incredibly popular and so there are a lot of resources you can tap into online. You can either:

* Just start coding and google specific questions when you get stuck (my preference for learning)
* Follow a few online tutorials (like this first)
* Do both!

**Here are a few resources:**
* [Jake VanderPlas Python Data Science Handbook](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)
  * [Buy the book](https://www.amazon.com/Python-Data-Science-Handbook-Essential-ebook-dp-B01N2JT3ST/dp/B01N2JT3ST/ref=mt_other?_encoding=UTF8&me=&qid=) for $35
  * Go through a series of [well documented Colab Notebooks](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb)
  * **Highly recommended resource!**
* [Code Academy: Analyze Data with Python](https://www.codecademy.com/learn/paths/analyze-data-with-python)
* [Udemy Academy: Data Analysis with Pandas & NumPy in Python for Beginner](https://www.udemy.com/course/python-data-analysis-2020/)
* [Datacamp: Introduction to Python](https://www.datacamp.com/courses/intro-to-python-for-data-science)


<img src="https://info.endaq.com/hubfs/datacamp-numpy-cheat-sheet.png" width="800">

<img src="https://info.endaq.com/hubfs/pandas-cheat-sheet1.png" width="800">

<img src="https://info.endaq.com/hubfs/pandas-cheat-sheet2.png" width="800">

### 3.2 NumPy

NumPy provides an in depth overview of [what exactly NumPy is](https://numpy.org/doc/stable/user/whatisnumpy.html). Quoting them:
>*At the core of the NumPy package, is the ndarray object. This encapsulates n-dimensional arrays of homogeneous data types, with many operations being performed in compiled code for performance.*

Let's get started by importing it, typically shortened to `np` because of how frequently it's called. This will come standard in most Python distributions such as Anaconda. If you need to install it simply:
~~~
!pip install numpy
~~~
"""

import numpy as np

"""#### 3.2.1 Creating Arrays

#### 3.2.2 Manually or from Lists
Manually create a list and demonstrate that operations on that list are difficult.
"""

lst = [0, 1, 2, 3]
lst * 2

"""That isn't what we expected! For lists, we need to loop through all elements to apply a function to them which can be incredibly time consuming."""

[i * 2 for i in lst]

"""Now lets make a numpy array and once in an array, let's show how intuitive operations are now that they are performed element by element."""

array = np.array(lst)
print(array)
print(array * 2)
print(array + 2)

"""Let's create a 2D matrix of 32 bit floats."""

np.array([[1, 0, 0],
          [0, 1, 0],
          [0, 0, 1]], dtype=np.float32)

"""#### 3.2.3 Using Functions

We'll go through a few here, but for more in depth examples and options see NumPy's [Array Creation Routines](https://numpy.org/doc/stable/reference/routines.array-creation.html). These first few examples are for very basic arrays/matrices.
"""

np.zeros(5)

np.zeros([2, 3])

np.eye(3)

"""Now let's start making sequences."""

np.arange(10)

np.arange(0,  #start
          10) #stop (not included in array)

np.arange(0,  #start
          10, #stop (not included in array)
          2)  #step size

np.linspace(0,  #start
            11, #stop (default to be included, can pass in endpoint=False)
            5)  #number of data points evenly spaced

np.logspace(0, #output starts being raised to this value
            4, #ending raised value
            4) #number of data points

"""Logspace is the equivalent of rasing a base by a linspaced array."""

10 ** np.linspace(0,3,4)

np.logspace(0, 4, 5, base=2)

"""If you don't want to have to do the mental math to know what exponent to raise the values to, you can use geomspace but this only helps for base of 10."""

np.geomspace(1, 1000, 4)

"""Random numbers!"""

np.random.rand(5)

np.random.rand(2, 3)

"""#### 3.2.4 Indexing

Let's first create a simple array.
"""

array = np.arange(1, 11, 1)
array

"""Now index the first item."""

array[0]

"""Index the last item."""

array[-1]

"""Grab every 2nd item."""

array[::2]

"""Grab every second item starting at index of 1 (the second value)"""

array[1::2]

"""Start from the second item, going to the 7th but skipping every other. (Our first time using the full `array[start (inclusive): stop (exclusive): step]` array 'slicing' notation)"""

array[1:6:2]

"""Reverse the order."""

array[::-1]

"""Boolean operations to index the array."""

array[array < 5]

array[array * 2 < 5]

"""Integer list can also index arrays."""

array[[1,3,5]]

"""Now let's create a slightly more complicated, 2 dimensional array."""

array_2d = np.arange(10).reshape((2, 5))
array_2d

"""Indexing the second dimension."""

array_2d[:, 1]

"""Any combination of these indexing methods works as well."""

array_2d[[True, False], 1::2]

array_2d[1, [0,1,4]]

"""#### 3.2.5 Operations"""

array

array + 100

array * 2

"""To raise all the elements in an array to an exponent we have to use the notation `**` not `^`."""

array ** 2

"""Use that shape to create a new array matching it to do operations with."""

array2 = np.arange(array.shape[0]) * 5
array2

array2 + array

"""#### 3.2.6 Stats of an Array"""

print(array)
print(array_2d)

print(array.shape)
print(array_2d.shape)

print(len(array))
print(len(array_2d))

print(array.max())
print(array_2d.max())

print(array.min())
print(array_2d.min())

array.std()

array.cumsum()

array.cumprod()

"""#### 3.2.7 Constants"""

np.pi

np.e

np.inf

"""#### 3.2.8 Functions"""

np.sin(np.pi / 2)

np.cos(np.pi)

np.log(np.e)

np.log10(100)

np.log2(64)

"""To demonstrate rounding, let's first make a new array with decimals."""

array = np.arange(4) / 3
print(array)
np.around(array, 2)

"""#### 3.2.9 Looping vs Vectorization

As mentioned in the beginning, NumPy uses machine code with their ndarray objects which is what leads to the performance improvements. Let's demonstrate this by constructing a simple sine wave.
"""

fs = 500 #sampling rate in Hz
d_t = 1 / fs #time steps in seconds
n_step = 1000 #number of steps (there will be n_step+1 data points)

amp = 1 #amplitude of sine wave
f = 2 #frequency of sine wave

time = np.linspace(0,             #start time
                   n_step * d_t,    #end time
                   num=n_step + 1)  #number of data points, one more than the steps to include an endpoint
time.shape

"""First we make a function to loop through each element and calculate the amplitude."""

def sine_wave_with_loop(time, amp, f, phase=0):
  length = time.shape[0]
  wave = np.zeros(length)

  for i in range(length-1):
    wave[i] = np.sin(2 * np.pi * f * time[i] + phase * np.pi / 180)*amp
  return wave

"""Now let's time how quickly that executes for our time array of 1,001 data points."""

# Commented out IPython magic to ensure Python compatibility.
# %timeit sine_wave_with_loop(time, amp, f)

"""Now let's do the same using NumPy's sine function and vectorization."""

def sine_wave_with_numpy(time, amp, f, phase=0):
  """Takes in a time array and sine wave parameters, returns an array of the sine wave amplitude."""
  return np.sin(2 * np.pi * f * time + phase * np.pi / 180) * amp

"""Notice my docstrings!"""

help(sine_wave_with_numpy)

# Commented out IPython magic to ensure Python compatibility.
# %timeit sine_wave_with_numpy(time, amp, f)

"""Using vectorization is about **100x faster!** And this increases the longer the loops are.

#### 3.2.10 Why Vectorization Works So Much Faster
The above example highlights that NumPy is much faster, but why? Because it is using compiled machine code under the hood for it's operations.

Python has the [Numba](http://numba.pydata.org/) package which can be used to do this compilation which we will do to highlight just why NumPy is faster (and recommended!).
"""

from numba import njit

numba_sine_wave_with_loop = njit(sine_wave_with_loop)
numba_sine_wave_with_numpy = njit(sine_wave_with_numpy)

# Commented out IPython magic to ensure Python compatibility.
# %timeit numba_sine_wave_with_loop(time, amp, f)
# %timeit numba_sine_wave_with_numpy(time, amp, f)

"""Let's combine this into a DataFrame we'll discuss next in more detail, but here's a preview."""

import pandas as pd

time_data = pd.DataFrame({'Time (us)':[2.77*100, 27.5, 23.1, 22.6],
                          'Method':['Loop','NumPy','Loop','NumPy'],
                          'Numba?':['w/o','w/o','w','w']}
)
time_data

"""Now let's plot it and preview Plotly!"""

!pip install --upgrade -q plotly
import plotly.express as px

fig = px.bar(time_data,
             x="Method",
             y="Time (us)",
             color="Numba?",
             title="Compute Time of a Sine Wave for 1000 Elements",
             barmode='group')
fig.show()

"""### 3.3 Pandas
Pandas is built *on top of* NumPy meaning that the data is stored still as NumPy ndarray objects under the hood. But it exposes a much more intuitive labeling/indexing architecture and allows you to link arrays of different types (strings, floats, integers etc.) to one another.

To quote Jake VanderPlas:
>*At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices.*

To start, import pandas as `pd`, again this will come standard in virtually all Python distributions such as Anaconda. But to install is simply:
~~~
!pip install pandas
~~~
"""

import pandas as pd

"""There are three types of Pandas objects, we'll only focus on the first two:
1. **Series** – 1D labeled homogeneous array, sizeimmutable
2. **Data Frames** – 2D labeled, size-mutable tabular structure with heterogenic columns
3. **Panel** – 3D labeled size mutable array.

#### 3.3.1 Creating a Series
First let's create a few numpy arrays.
"""

amplitude = sine_wave_with_numpy(time, amp, f, 180)
print(time)
print(amplitude)

"""Now let's see what a series looks like made from one of the arrays."""

pd.Series(amplitude)

"""This type of series has some value, but you really start to see it when you add in an index."""

series = pd.Series(data=amplitude,
                   index=time,
                   name='Amplitude')
series

"""Here's where Pandas shines - indexing is much more intuitive (and inclusive) to specify based on labels, not those confusing integer locations. We'll come back to this when we have the dataframe next too."""

series[0: 0.01]

"""Being able to plot quickly is also a plus!"""

series.plot()

"""Remember we never left the NumPy array, it is still here and can be accessed with the following."""

series.values

series.to_numpy()

"""#### 3.3.2 Creating a DataFrame

A DataFrame is basically a sequence of aligned series objects, and by aligned I mean they share a common index or label. This let's us mix and match types easily among other benefits.

First we'll start creating dataframes using what is called a "dictionary" with keys and values.
"""

df = pd.DataFrame({"Phase 0": sine_wave_with_numpy(time, amp, f, 00),
                   "Phase 90": sine_wave_with_numpy(time, amp, f, 90),
                   "Phase 180": sine_wave_with_numpy(time, amp, f, 180),
                   "Phase 270": sine_wave_with_numpy(time, amp, f, 270)},
                  index=time)
df

"""#### 3.3.3 Plotting (Preview)

Dataframes also wrap around Matplotlib to allow for plotting directly from the dataframe object itself. This can also be done from the Pandas Series object too like we showed earlier.
"""

df.plot()

df['Max'] = df.max(axis=1)
df['Min'] = df.min(axis=1)
df

"""This will be the topic of the next webinar, plotting with Plotly!

Note that I need to install an upgraded version of Plotly in Colab because the default Plotly Express version doesn't work in Colab (but their more advanced graph objects does).
"""

!pip install --upgrade -q plotly
import plotly.express as px

px.line(df).show()

"""#### 3.3.4 Load from CSV

This dataset was discussed in a blog on [vibration metrics and used bearing data as an example](https://blog.endaq.com/top-vibration-metrics-to-monitor-how-to-calculate-them).

Note you don't *have* to use a CSV. They have a lot of other file formats natively supported ([see full list](https://pandas.pydata.org/docs/user_guide/io.html)):
* hdf
* feather
* pickle

But I know everyone likes CSVs!
"""

df = pd.read_csv('https://info.endaq.com/hubfs/Plots/bearing_data.csv', index_col=0)
df

"""#### 3.3.5 Save CSV

Like reading data, there are a host of native formats we can save data from a dataframe. [See documentation](https://pandas.pydata.org/docs/reference/io.html).
"""

df.to_csv('bearing-data.csv')

"""#### 3.3.6 Simple Analysis"""

df.describe()

df.std()

df.max()

"""Note that these built in Pandas functions are using NumPy to process and are the equivalent of doing the following."""

np.max(df)

df.quantile(0.25)

df['abs(max)'] = df.abs().max(axis=1)
df

"""#### 3.3.7 Indexing
Here is where indexing in Python gets a whole lot more intuitive! A dataframe with an index let's use index values (time in this case) to slice the dataframe, not rely on the nth element in the arrays.
"""

df[0: 0.05]

"""We can also use the same convention as before by adding in a step definition, in this case we'll grab every 100th point."""

df[0: 0.05: 100]

"""There are ways to use the integer based indexing if you so desire."""

df.iloc[0:10]

"""#### 3.3.8 Rolling
I love the [rolling method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) which allows for easy rolling window calculations, something you'll do frequently with time series data.
"""



df.rolling(n).max()[::n]

px.line(df.rolling(n).max()[::n]).show()

"""#### 3.3.9 Datetime Data (Yay Finance!)

Let's use Yahoo Finance and stock data as a relatable example of data with datetimes.
"""

!pip install -q yfinance
import yfinance as yf

df =  yf.download(["SPY", "AAPL", "MSFT", "AMZN", "GOOGL"],
                  start='2019-01-01',
                  end='2021-09-24')
df

"""Let's compare not the price, but the relative performance."""

df = df['Adj Close']
df = df / df.iloc[0]
df

px.line(df).show()

"""The [rolling function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) will play very nicely with datetime data as shown here when I get the moving average over a 40 day period. And this can handle unevenly sampled date easily."""

px.line(df.rolling('40d').mean()).show()

"""Indexing with datetime data though will require a slightly extra step, but then it is easy."""

from datetime import date
start = date(2021, 4, 1)
end = date(2021, 4, 30)

df[start:end]

pd.date_range(start='1/1/2019', end='08/31/2021', freq='M')

df.resample(rule='Q').max()

"""#### 3.3.10 Sorting & Filtering on Tabular Data
To highlight filtering in DataFrames, we'll use a dataset with a bunch of different columns/series of different types. This data was pulled directly from the enDAQ cloud API off some example recording files.
"""

df = pd.read_csv('https://info.endaq.com/hubfs/data/endaq-cloud-table.csv')
df

df.columns

"""There's a lot of data here! So we'll focus on just a handful of columns and convert the time in seconds to a datetime object."""

df = df[['serial_number_id', 'file_name', 'file_size', 'recording_length', 'recording_ts',
         'accelerationPeakFull', 'psuedoVelocityPeakFull', 'accelerationRMSFull',
         'velocityRMSFull', 'displacementRMSFull', 'pressureMeanFull', 'temperatureMeanFull']].copy()

df['recording_ts'] = pd.to_datetime(df['recording_ts'], unit='s')
df = df.sort_values(by=['recording_ts'], ascending=False)
df

"""Filtering is made simple with boolean expressions that can be combined. There is also a method to sort_values by columns/series."""

mask = df.recording_ts > pd.to_datetime('2021-01-01')
df[mask].sort_values(by=['serial_number_id'], ascending=False)

mask = (df.recording_ts > pd.to_datetime('2021-01-01')) & (df.accelerationPeakFull > 100)
df[mask].sort_values(by=['accelerationPeakFull'], ascending=False)

"""Another preview to plotly, but visualizing dataframes is made easy, even with mixed types."""

px.scatter(df,
           x="recording_ts",
           y="accelerationRMSFull",
           size="recording_length",
           color="serial_number_id",
           hover_name="file_name",
           log_y=True,
           size_max=60).show()

"""Plotly automatically made my colors a colorbar because I specified it based on a numeric value. If instead I change the type to string and replot, we'll see discrete series for each device."""

df['device'] = df["serial_number_id"].astype(str)

px.scatter(df,
           x="recording_ts",
           y="accelerationRMSFull",
           size="recording_length",
           color="device",
           hover_name="file_name",
           log_y=True,
           size_max=60).show()

"""## 4. Supplementary Activity

Download the dataset attached on the canvas course and answer the following:

1. Identify the column names
2. Identify the data types of the data
3. Display the total number of records
4. Display the first 20 records
5. Display the last 20 records
6. Change the Outcome column to Diagnosis
7. Create a new column Classification that display "Diabetes" if the value of outcome is 1 , otherwise "No Diabetes"
8. Create a new dataframe "withDiabetes" that gathers data with diabetes
9. Create a new dataframe "noDiabetes" thats gathers data with no diabetes
10. Create a new dataframe "Pedia" that gathers data with age 0 to 19
11. Create a new dataframe "Adult" that gathers data with age greater than 19
12. Use numpy to get the average age and glucose value.
13. Use numpy to get the median age and glucose value.
14. Use numpy to get the middle values of glucose and age.
15. Use numpy to get the standard deviation of the skinthickness.
"""

df = pd.read_csv('/content/diabetes.csv')
df

#1
df.columns

#2
df.dtypes

#3
print(df.shape[0])

#4
print(df.head(20))

#5
print(df.tail(20))

#6
df_diagnosis = df.rename(columns={'Outcome': 'Diagnosis'})
df_diagnosis

#7
df_diagnosis['Classification'] = np.where(df_diagnosis['Diagnosis'] == 1, 'Diabetes', 'No Diabetes')
df_diagnosis

#8
df_diabetes = df_diagnosis[df_diagnosis['Diagnosis'] == 1]
df_diabetes

#9
df_nodiabetes = df_diagnosis[df_diagnosis['Diagnosis'] == 0]
df_nodiabetes

#10
df_pedia = df_diagnosis[(df_diagnosis['Age'] >= 0) & (df_diagnosis['Age'] <= 19)]
df

#11
df_adult = df_diagnosis[(df_diagnosis['Age'] > 19)]
df_adult

#12
print(np.mean(df_diagnosis['Age']))
print(np.mean(df_diagnosis['Glucose']))

#13
print(np.median(df_diagnosis['Age']))
print(np.median(df_diagnosis['Glucose']))

#14
print(np.percentile(df_diagnosis['Age'], 50))

#15
print(np.std(df_diagnosis['SkinThickness']))

"""## 5. Summary, Conclusions and Lessons Learned

To summarize my learnings, this activity teaches us how to use numpy and panda and its importance when manipulating and visualizing data. Numpy is the core library for computing in Python. It provides a high-performance multidimensional array object, and tools for working on the said arrays. Pandas is a library used for working with data. Dataframes are at the center of pandas. A dataframe is structured like a table or spreadsheet. The row and columns both have indexes, and it allows you to perform operations on rows/columns separately.


During this activity, I refreshed my memory on how to manipulate csv files and visualize data based on my previous learnings in my course VDA. I remembered how to rename columns into my desired name, create a new column and replace the values with my desired value, and view data based on my desired age group.

<hr/>

***Proprietary Clause***

*Property of the Technological Institute of the Philippines (T.I.P.). No part of the materials made and uploaded in this learning management system by T.I.P. may be copied, photographed, printed, reproduced, shared, transmitted, translated, or reduced to any electronic medium or machine-readable form, in whole or in part, without the prior consent of T.I.P.*
"""